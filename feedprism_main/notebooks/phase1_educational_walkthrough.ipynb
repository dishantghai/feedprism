{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1: Core Pipeline - Educational Walkthrough\n",
        "\n",
        "This notebook walks through the complete Phase 1 pipeline step-by-step.\n",
        "\n",
        "**What you'll learn:**\n",
        "1. How to fetch emails from Gmail API\n",
        "2. How to parse HTML newsletters\n",
        "3. How to extract events using LLMs\n",
        "4. How to generate embeddings\n",
        "5. How to store in Qdrant vector database\n",
        "6. How to search semantically\n",
        "\n",
        "**Prerequisites:**\n",
        "- Phase 0 complete (environment setup)\n",
        "- Gmail OAuth configured\n",
        "- Qdrant running in Docker\n",
        "- OpenAI API key set in `.env`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Playground: Validate Functionalities here \n",
        "- Add Individual Scripts in the Actual Project Path \n",
        "- Run code to Understand Each File / Module\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Create & Test Gmail Client (app/services/gmail_client.py) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:35: SyntaxWarning: invalid escape sequence '\\S'\n",
            "<>:41: SyntaxWarning: invalid escape sequence '\\T'\n",
            "<>:47: SyntaxWarning: invalid escape sequence '\\F'\n",
            "<>:35: SyntaxWarning: invalid escape sequence '\\S'\n",
            "<>:41: SyntaxWarning: invalid escape sequence '\\T'\n",
            "<>:47: SyntaxWarning: invalid escape sequence '\\F'\n",
            "/var/folders/l5/vvzf291d5cx881b14hn6pc200000gn/T/ipykernel_59738/3551444189.py:35: SyntaxWarning: invalid escape sequence '\\S'\n",
            "  print(f\"\\Second email:\")\n",
            "/var/folders/l5/vvzf291d5cx881b14hn6pc200000gn/T/ipykernel_59738/3551444189.py:41: SyntaxWarning: invalid escape sequence '\\T'\n",
            "  print(f\"\\Third email:\")\n",
            "/var/folders/l5/vvzf291d5cx881b14hn6pc200000gn/T/ipykernel_59738/3551444189.py:47: SyntaxWarning: invalid escape sequence '\\F'\n",
            "  print(f\"\\Fourth email:\")\n",
            "\u001b[32m2025-11-26 16:05:02.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.services.gmail_client\u001b[0m:\u001b[36m_authenticate\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mLoading Gmail token from cache\u001b[0m\n",
            "\u001b[32m2025-11-26 16:05:02.844\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mapp.services.gmail_client\u001b[0m:\u001b[36m_authenticate\u001b[0m:\u001b[36m67\u001b[0m - \u001b[32m\u001b[1mGmail API authenticated successfully\u001b[0m\n",
            "\u001b[32m2025-11-26 16:05:02.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.services.gmail_client\u001b[0m:\u001b[36mfetch_content_rich_emails\u001b[0m:\u001b[36m200\u001b[0m - \u001b[1mFetching content-rich emails (last 7 days)\u001b[0m\n",
            "\u001b[32m2025-11-26 16:05:02.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.services.gmail_client\u001b[0m:\u001b[36mlist_messages\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1mListing messages with query: 'newer_than:7d (unsubscribe OR newsletter OR event OR webinar OR course)'\u001b[0m\n",
            "\u001b[32m2025-11-26 16:05:03.261\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mapp.services.gmail_client\u001b[0m:\u001b[36mlist_messages\u001b[0m:\u001b[36m100\u001b[0m - \u001b[32m\u001b[1mFound 5 messages\u001b[0m\n",
            "\u001b[32m2025-11-26 16:05:03.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.services.gmail_client\u001b[0m:\u001b[36mget_messages_batch\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mFetching 5 messages\u001b[0m\n",
            "\u001b[32m2025-11-26 16:05:05.357\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mapp.services.gmail_client\u001b[0m:\u001b[36mget_messages_batch\u001b[0m:\u001b[36m133\u001b[0m - \u001b[32m\u001b[1mFetched 5 messages\u001b[0m\n",
            "\u001b[32m2025-11-26 16:05:05.360\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mapp.services.gmail_client\u001b[0m:\u001b[36mfetch_content_rich_emails\u001b[0m:\u001b[36m230\u001b[0m - \u001b[32m\u001b[1mSuccessfully parsed 5 emails\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Fetched 5 emails\n",
            "\n",
            "First email:\n",
            "  Subject: Your guide to building apps with Google Antigravity\n",
            "  From: Michael Spencer and Jeff Morhous from AI Supremacy  <aisupremacy+guides@substack.com>\n",
            "  Has HTML: True\n",
            "\\Second email:\n",
            "  Subject: üò∫ Why is the US building an AI Manhattan Project?\n",
            "  From: The Neuron <theneuron@newsletter.theneurondaily.com>\n",
            "  Has HTML: True\n",
            "\\Third email:\n",
            "  Subject: Get ready for all things tech and its future ‚ú®\n",
            "  From: upGrad <info@updates.upgrad.com>\n",
            "  Has HTML: True\n",
            "\\Fourth email:\n",
            "  Subject: Hey, we‚Äôve prepared your MSME brief with the ‚Çπ10,00,000 cap\n",
            "  From: Flexi Loan Desk <info@pool.driftem.com>\n",
            "  Has HTML: True\n"
          ]
        }
      ],
      "source": [
        "# Test Gmail client\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "#sys.path.insert(0, '/Users/Shared/ALL WORKSPACE/Hackathons/mom_hack/feedprism_main')\n",
        "# Get the absolute path to the project root (feedprism_main)\n",
        "# Assuming the notebook is in feedprism_main/notebooks/\n",
        "project_root = Path.cwd().parent.absolute()\n",
        "\n",
        "# Change working directory to project root\n",
        "os.chdir(project_root)\n",
        "\n",
        "# Add project root to Python path\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "from app.services.gmail_client import GmailClient\n",
        "\n",
        "# Initialize client\n",
        "client = GmailClient()\n",
        "\n",
        "# Fetch 5 recent emails\n",
        "emails = client.fetch_content_rich_emails(days_back=7, max_results=5)\n",
        "\n",
        "print(f\"\\n‚úÖ Fetched {len(emails)} emails\")\n",
        "if emails:\n",
        "    first = emails[0]\n",
        "    print(f\"\\nFirst email:\")\n",
        "    print(f\"  Subject: {first['subject']}\")\n",
        "    print(f\"  From: {first['from']}\")\n",
        "    print(f\"  Has HTML: {first['body_html'] is not None}\")\n",
        "\n",
        "    second = emails[1]\n",
        "    print(f\"\\Second email:\")\n",
        "    print(f\"  Subject: {second['subject']}\")\n",
        "    print(f\"  From: {second['from']}\")\n",
        "    print(f\"  Has HTML: {second['body_html'] is not None}\")\n",
        "\n",
        "    third = emails[2]\n",
        "    print(f\"\\Third email:\")\n",
        "    print(f\"  Subject: {third['subject']}\")\n",
        "    print(f\"  From: {third['from']}\")\n",
        "    print(f\"  Has HTML: {third['body_html'] is not None}\")\n",
        "\n",
        "    fourth = emails[3]\n",
        "    print(f\"\\Fourth email:\")\n",
        "    print(f\"  Subject: {fourth   ['subject']}\")\n",
        "    print(f\"  From: {fourth['from']}\")\n",
        "    print(f\"  Has HTML: {fourth['body_html'] is not None}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Create & Test Email Parsing \n",
        "- (app/services/parser.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-26 16:05:10.907\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mapp.services.parser\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m59\u001b[0m - \u001b[34m\u001b[1mEmailParser initialized with html2text converter\u001b[0m\n",
            "\u001b[32m2025-11-26 16:05:10.911\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mapp.services.parser\u001b[0m:\u001b[36m_clean_html\u001b[0m:\u001b[36m178\u001b[0m - \u001b[34m\u001b[1mHTML cleaned: removed scripts, styles, tracking pixels\u001b[0m\n",
            "\u001b[32m2025-11-26 16:05:10.911\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mapp.services.parser\u001b[0m:\u001b[36m_extract_links\u001b[0m:\u001b[36m260\u001b[0m - \u001b[34m\u001b[1mExtracted 2 valid links\u001b[0m\n",
            "\u001b[32m2025-11-26 16:05:10.912\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mapp.services.parser\u001b[0m:\u001b[36mparse_html_email\u001b[0m:\u001b[36m108\u001b[0m - \u001b[32m\u001b[1mParsed email: 201 chars, 2 links\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ HTML Parser Test Results:\n",
            "\n",
            "üìß Title: AI Weekly Newsletter\n",
            "\n",
            "üìù Text Length: 201 characters\n",
            "\n",
            "üîó Links Found: 2\n",
            "   1. Register here ‚Üí https://example.com/register\n",
            "   2. The Future of LLMs ‚Üí https://example.com/article\n",
            "\n",
            "üìÑ Clean Text Preview:\n",
            "# This Week in AI\n",
            "\n",
            "Join us for the **AI Summit 2025** on December 15th.\n",
            "\n",
            "[Register here](https://example.com/register)\n",
            "\n",
            "## Featured Article\n",
            "\n",
            "Read about [The Future of LLMs](https://example.com/article)\n",
            "\n",
            "‚úÖ All assertions passed!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/Users/Shared/ALL WORKSPACE/Hackathons/mom_hack/feedprism_main')\n",
        "\n",
        "from app.services.parser import EmailParser\n",
        "\n",
        "# Initialize parser\n",
        "parser = EmailParser()\n",
        "\n",
        "# Test with sample newsletter HTML\n",
        "sample_html = \"\"\"\n",
        "<html>\n",
        "<head><title>AI Weekly Newsletter</title></head>\n",
        "<body>\n",
        "    <h1>This Week in AI</h1>\n",
        "    <p>Join us for the <strong>AI Summit 2025</strong> on December 15th.</p>\n",
        "    <p><a href=\"https://example.com/register\">Register here</a></p>\n",
        "    \n",
        "    <h2>Featured Article</h2>\n",
        "    <p>Read about <a href=\"https://example.com/article\">The Future of LLMs</a></p>\n",
        "    \n",
        "    <!-- Tracking pixel -->\n",
        "    <img src=\"https://track.example.com/pixel.gif\" width=\"1\" height=\"1\">\n",
        "    \n",
        "    <footer style=\"font-size:10px\">\n",
        "        <a href=\"https://example.com/unsubscribe\">Unsubscribe</a>\n",
        "    </footer>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Parse\n",
        "result = parser.parse_html_email(sample_html)\n",
        "\n",
        "print(\"‚úÖ HTML Parser Test Results:\")\n",
        "print(f\"\\nüìß Title: {result['title']}\")\n",
        "print(f\"\\nüìù Text Length: {len(result['text'])} characters\")\n",
        "print(f\"\\nüîó Links Found: {len(result['links'])}\")\n",
        "\n",
        "for i, link in enumerate(result['links'], 1):\n",
        "    print(f\"   {i}. {link['text']} ‚Üí {link['url']}\")\n",
        "\n",
        "print(f\"\\nüìÑ Clean Text Preview:\")\n",
        "print(result['text'][:300])\n",
        "\n",
        "# Verify tracking pixel was removed\n",
        "assert 'pixel.gif' not in result['text'], \"Tracking pixel should be removed\"\n",
        "# Verify unsubscribe footer was removed\n",
        "assert 'unsubscribe' not in result['text'].lower(), \"Unsubscribe footer should be removed\"\n",
        "# Verify links were preserved\n",
        "assert len(result['links']) >= 2, \"Should extract at least 2 links\"\n",
        "\n",
        "print(\"\\n‚úÖ All assertions passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Create & Test 'Event' Data Model \n",
        "- app/models/extraction.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Event Data Models Test\n",
            "\n",
            "1. Created event: Machine Learning Workshop\n",
            "   Status: EventStatus.UPCOMING\n",
            "\n",
            "2. ‚úÖ JSON Schema generated: 7 keys\n",
            "\n",
            "‚úÖ All model tests passed!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/Users/Shared/ALL WORKSPACE/Hackathons/mom_hack/feedprism_main')\n",
        "\n",
        "from app.models.extraction import ExtractedEvent, EventExtractionResult, EventStatus\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "print(\"‚úÖ Event Data Models Test\\n\")\n",
        "\n",
        "# Test 1: Create valid event\n",
        "event = ExtractedEvent(\n",
        "    title=\"Machine Learning Workshop\",\n",
        "    description=\"Hands-on workshop covering neural networks\",\n",
        "    start_time=\"2025-12-20T14:00:00\",\n",
        "    location=\"Online\",\n",
        "    tags=[\"ML\", \"Workshop\"]\n",
        ")\n",
        "\n",
        "print(f\"1. Created event: {event.title}\")\n",
        "print(f\"   Status: {event.compute_status()}\")\n",
        "\n",
        "# Test 2: JSON Schema generation\n",
        "schema = EventExtractionResult.model_json_schema()\n",
        "print(f\"\\n2. ‚úÖ JSON Schema generated: {len(schema)} keys\")\n",
        "\n",
        "print(\"\\n‚úÖ All model tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Test LLM Extraction of Events from a Given Email"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-26 16:05:15.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.services.extractor\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mLLMExtractor initialized: model=gpt-4o-mini\u001b[0m\n",
            "\u001b[32m2025-11-26 16:05:15.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.services.extractor\u001b[0m:\u001b[36mextract_events\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mExtracting events from: 'AI Summit 2025...'\u001b[0m\n",
            "\u001b[32m2025-11-26 16:05:18.914\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mapp.services.extractor\u001b[0m:\u001b[36mextract_events\u001b[0m:\u001b[36m64\u001b[0m - \u001b[32m\u001b[1mExtracted 1 events (confidence: 1.00)\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Extracted 1 events\n",
            "   Confidence: 1.00\n",
            "   Title: AI Summit 2025\n",
            "   Location: Online\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../')  # Ensure project root is in path\n",
        "from app.services.extractor import LLMExtractor\n",
        "async def test():\n",
        "    extractor = LLMExtractor()\n",
        "    \n",
        "    email = \"\"\"\n",
        "    Join us for the AI Summit 2025!\n",
        "    Date: December 15, 2025 at 2:00 PM EST\n",
        "    Location: Online\n",
        "    Register: https://example.com/register\n",
        "    Free for students, $50 for professionals.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Note: Ensure OPENAI_API_KEY is set in .env\n",
        "    result = await extractor.extract_events(email, \"AI Summit 2025\")\n",
        "    \n",
        "    print(f\"‚úÖ Extracted {len(result.events)} events\")\n",
        "    print(f\"   Confidence: {result.confidence:.2f}\")\n",
        "    \n",
        "    if result.events:\n",
        "        event = result.events[0]\n",
        "        print(f\"   Title: {event.title}\")\n",
        "        print(f\"   Location: {event.location}\")\n",
        "# Run the test directly\n",
        "await test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Create & Verify Embedding Generation\n",
        "- app/services/embedding_service.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/Shared/ALL WORKSPACE/Hackathons/mom_hack/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "\u001b[32m2025-11-26 16:23:58.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.services.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mLoading embedding model: sentence-transformers/all-MiniLM-L6-v2\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Embedding Service Test\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-26 16:24:02.107\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mapp.services.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m29\u001b[0m - \u001b[32m\u001b[1mModel loaded: 384D vectors\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Generated embedding:\n",
            "   Dimension: 384\n",
            "   First 5 values: [-0.1260364055633545, -0.025976352393627167, 0.050567951053380966, -0.012599319219589233, 0.02731657400727272]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-26 16:24:05.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.services.embedder\u001b[0m:\u001b[36membed_batch\u001b[0m:\u001b[36m71\u001b[0m - \u001b[1mEmbedding 3 texts in batches of 32\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "2. Semantic similarity:\n",
            "   AI conference ‚Üî ML workshop: 0.636\n",
            "   AI conference ‚Üî Cooking: 0.046\n",
            "\n",
            "3. Batch embedding:\n",
            "   Generated 3 vectors\n",
            "\n",
            "‚úÖ All tests passed!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/Users/Shared/ALL WORKSPACE/Hackathons/mom_hack/feedprism_main')\n",
        "\n",
        "from app.services.embedder import EmbeddingService\n",
        "import numpy as np\n",
        "\n",
        "print(\"‚úÖ Embedding Service Test\\n\")\n",
        "\n",
        "embedder = EmbeddingService()\n",
        "\n",
        "# Test 1: Single embedding\n",
        "text = \"Machine learning workshop on neural networks\"\n",
        "vec = embedder.embed_text(text)\n",
        "\n",
        "print(f\"1. Generated embedding:\")\n",
        "print(f\"   Dimension: {len(vec)}\")\n",
        "print(f\"   First 5 values: {vec[:5]}\")\n",
        "\n",
        "# Test 2: Semantic similarity\n",
        "text1 = \"AI conference about deep learning\"\n",
        "text2 = \"Machine learning workshop\"\n",
        "text3 = \"Cooking recipes for dinner\"\n",
        "\n",
        "vec1 = np.array(embedder.embed_text(text1))\n",
        "vec2 = np.array(embedder.embed_text(text2))\n",
        "vec3 = np.array(embedder.embed_text(text3))\n",
        "\n",
        "# Cosine similarity\n",
        "sim_12 = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "sim_13 = np.dot(vec1, vec3) / (np.linalg.norm(vec1) * np.linalg.norm(vec3))\n",
        "\n",
        "print(f\"\\n2. Semantic similarity:\")\n",
        "print(f\"   AI conference ‚Üî ML workshop: {sim_12:.3f}\")\n",
        "print(f\"   AI conference ‚Üî Cooking: {sim_13:.3f}\")\n",
        "\n",
        "assert sim_12 > sim_13, \"Related texts should be more similar\"\n",
        "\n",
        "# Test 3: Batch embedding\n",
        "texts = [\"Event 1\", \"Event 2\", \"Event 3\"]\n",
        "vecs = embedder.embed_batch(texts)\n",
        "\n",
        "print(f\"\\n3. Batch embedding:\")\n",
        "print(f\"   Generated {len(vecs)} vectors\")\n",
        "\n",
        "print(\"\\n‚úÖ All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Create and initialize Qdrant client\n",
        "- app/database/qdrant_client.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-26 17:33:01.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.database.qdrant_client\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mConnecting to Qdrant: localhost:6333\u001b[0m\n",
            "\u001b[32m2025-11-26 17:33:01.802\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mapp.database.qdrant_client\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m43\u001b[0m - \u001b[32m\u001b[1mQdrant client initialized\u001b[0m\n",
            "\u001b[32m2025-11-26 17:33:01.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.services.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mLoading embedding model: sentence-transformers/all-MiniLM-L6-v2\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Qdrant Service Test\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-26 17:33:06.006\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mapp.services.embedder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m29\u001b[0m - \u001b[32m\u001b[1mModel loaded: 384D vectors\u001b[0m\n",
            "\u001b[32m2025-11-26 17:33:06.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.database.qdrant_client\u001b[0m:\u001b[36mcreate_collection\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mCreating collection: feedprism_emails\u001b[0m\n",
            "\u001b[32m2025-11-26 17:33:06.632\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mapp.database.qdrant_client\u001b[0m:\u001b[36mcreate_collection\u001b[0m:\u001b[36m67\u001b[0m - \u001b[32m\u001b[1mCollection created: feedprism_emails\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. ‚úÖ Collection created\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-11-26 17:33:06.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.database.qdrant_client\u001b[0m:\u001b[36mupsert_points\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mUpserting 4 points to feedprism_emails\u001b[0m\n",
            "\u001b[32m2025-11-26 17:33:07.153\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mapp.database.qdrant_client\u001b[0m:\u001b[36mupsert_points\u001b[0m:\u001b[36m89\u001b[0m - \u001b[32m\u001b[1mUpserted 4 points\u001b[0m\n",
            "\u001b[32m2025-11-26 17:33:07.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.database.qdrant_client\u001b[0m:\u001b[36msearch\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mSearching feedprism_emails (limit=3)\u001b[0m\n",
            "\u001b[32m2025-11-26 17:33:07.309\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mapp.database.qdrant_client\u001b[0m:\u001b[36msearch\u001b[0m:\u001b[36m139\u001b[0m - \u001b[32m\u001b[1mFound 3 results\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2. ‚úÖ Upserted 4 points\n",
            "\n",
            "3. Search results:\n",
            "   1. Engineering Machine tools Learning Event (score: 0.743)\n",
            "   2. AI Conference on Machine Learning (score: 0.574)\n",
            "   3. Data Science Meetup (score: 0.367)\n",
            "\n",
            "4. Collection info:\n",
            "   Points: 4\n",
            "   Status: green\n",
            "\n",
            "‚úÖ All tests passed!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/Users/Shared/ALL WORKSPACE/Hackathons/mom_hack/feedprism_main')\n",
        "\n",
        "from app.database.qdrant_client import QdrantService\n",
        "from app.services.embedder import EmbeddingService\n",
        "from qdrant_client.models import PointStruct\n",
        "import uuid\n",
        "\n",
        "print(\"‚úÖ Qdrant Service Test\\n\")\n",
        "\n",
        "# Initialize services\n",
        "qdrant = QdrantService()\n",
        "embedder = EmbeddingService()\n",
        "\n",
        "# Create collection\n",
        "qdrant.create_collection(recreate=True)\n",
        "print(\"1. ‚úÖ Collection created\")\n",
        "\n",
        "# Create test points\n",
        "texts = [\n",
        "    \"AI Conference on Machine Learning\",\n",
        "    \"Python Workshop for Beginners\",\n",
        "    \"Data Science Meetup\",\n",
        "    \"Engineering Machine tools Learning Event\"\n",
        "]\n",
        "\n",
        "points = []\n",
        "for text in texts:\n",
        "    vec = embedder.embed_text(text)\n",
        "    point = PointStruct(\n",
        "        id=str(uuid.uuid4()),\n",
        "        vector=vec,\n",
        "        payload={\"title\": text, \"type\": \"event\"}\n",
        "    )\n",
        "    points.append(point)\n",
        "\n",
        "# Upsert points\n",
        "qdrant.upsert_points(points)\n",
        "print(f\"2. ‚úÖ Upserted {len(points)} points\")\n",
        "\n",
        "# Search\n",
        "query_vec = embedder.embed_text(\"machine learning event\")\n",
        "results = qdrant.search(query_vec, limit=3)\n",
        "\n",
        "print(f\"\\n3. Search results:\")\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"   {i}. {result['payload']['title']} (score: {result['score']:.3f})\")\n",
        "\n",
        "# Collection info\n",
        "info = qdrant.get_collection_info()\n",
        "print(f\"\\n4. Collection info:\")\n",
        "print(f\"   Points: {info['points_count']}\")\n",
        "print(f\"   Status: {info['status']}\")\n",
        "\n",
        "print(\"\\n‚úÖ All tests passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Import Libraries and Initialize\n",
        "\n",
        "First, let's import all the modules we'll need and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Working directory: /Users/Shared/ALL WORKSPACE/Hackathons/mom_hack/feedprism_main\n",
            "‚úÖ All imports successful!\n",
            "üìÅ Project root: /Users/Shared/ALL WORKSPACE/Hackathons/mom_hack/feedprism_main\n",
            "üîë Credentials path: credentials.json\n",
            "üé´ Token path: token.json\n",
            "üîß Config loaded: OpenAI key = ***JfoA\n"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import sys\n",
        "import json\n",
        "import os\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# IMPORTANT: Change working directory to project root\n",
        "# This ensures credentials.json and token.json are found correctly\n",
        "project_root = Path('/Users/Shared/ALL WORKSPACE/Hackathons/mom_hack/feedprism_main')\n",
        "os.chdir(project_root)\n",
        "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Add project root to Python path\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Third-party imports\n",
        "import asyncio\n",
        "from loguru import logger\n",
        "\n",
        "# Our custom modules (we'll build these step by step)\n",
        "from app.config import settings\n",
        "from app.services.gmail_client import GmailClient\n",
        "from app.services.parser import EmailParser\n",
        "from app.services.extractor import LLMExtractor\n",
        "from app.services.embedder import EmbeddingService\n",
        "from app.database.qdrant_client import QdrantService\n",
        "from app.models.extraction import ExtractedEvent, EventExtractionResult\n",
        "\n",
        "# Configure logger for notebook\n",
        "logger.remove()  # Remove default handler\n",
        "logger.add(sys.stdout, level=\"INFO\", format=\"<level>{level: <8}</level> | {message}\")\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"üìÅ Project root: {project_root}\")\n",
        "print(f\"üîë Credentials path: {settings.gmail_credentials_path}\")\n",
        "print(f\"üé´ Token path: {settings.gmail_token_path}\")\n",
        "print(f\"üîß Config loaded: OpenAI key = {'***' + settings.openai_api_key[-4:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Fetch Emails from Gmail\n",
        "\n",
        "**What's happening:**\n",
        "- We connect to Gmail API using OAuth credentials\n",
        "- Search for content-rich emails (newsletters, events)\n",
        "- Fetch full email content including HTML body\n",
        "\n",
        "**Key concepts:**\n",
        "- Gmail API uses labels and search queries\n",
        "- Emails have multipart structure (text + HTML)\n",
        "- We prefer HTML for newsletters (richer content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mINFO    \u001b[0m | Loading Gmail token from cache\n",
            "\u001b[1mINFO    \u001b[0m | Refreshing expired Gmail token\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Gmail API authenticated successfully\n",
            "‚úÖ Gmail client initialized\n",
            "\n",
            "üìß Fetching emails...\n",
            "\u001b[1mINFO    \u001b[0m | Fetching content-rich emails (last 7 days)\n",
            "\u001b[1mINFO    \u001b[0m | Listing messages with query: 'newer_than:7d (unsubscribe OR newsletter OR event OR webinar OR course)'\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Found 5 messages\n",
            "\u001b[1mINFO    \u001b[0m | Fetching 5 messages\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Fetched 5 messages\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Successfully parsed 5 emails\n",
            "\n",
            "‚úÖ Fetched 5 emails\n",
            "\n",
            "üì¨ First Email Details:\n",
            "  Subject: Fwd: üîÆ Ilya Sutskever breaks silence on AI's future\n",
            "  From: dishant ghai <dishant.ghai@gmail.com>\n",
            "  Date: 2025-11-26T17:09:59+05:30\n",
            "  Has HTML: True\n",
            "  HTML Length: 78230 characters\n",
            "  Snippet: ---------- Forwarded message --------- From: The Rundown AI &lt;news@daily.therundown.ai&gt; Date: W...\n"
          ]
        }
      ],
      "source": [
        "# Initialize Gmail client\n",
        "# This will:\n",
        "# 1. Load OAuth credentials from token.json\n",
        "# 2. Refresh token if expired\n",
        "# 3. Connect to Gmail API\n",
        "gmail_client = GmailClient()\n",
        "\n",
        "print(\"‚úÖ Gmail client initialized\")\n",
        "print(\"\\nüìß Fetching emails...\")\n",
        "\n",
        "# Fetch content-rich emails from last 7 days\n",
        "# Parameters:\n",
        "#   days_back: How far back to search\n",
        "#   max_results: Maximum emails to fetch\n",
        "emails = gmail_client.fetch_content_rich_emails(\n",
        "    days_back=7,\n",
        "    max_results=5  # Start with just 5 for this demo\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Fetched {len(emails)} emails\")\n",
        "\n",
        "# Let's examine the first email\n",
        "if emails:\n",
        "    first_email = emails[0]\n",
        "    print(\"\\nüì¨ First Email Details:\")\n",
        "    print(f\"  Subject: {first_email['subject']}\")\n",
        "    print(f\"  From: {first_email['from']}\")\n",
        "    print(f\"  Date: {first_email['date']}\")\n",
        "    print(f\"  Has HTML: {first_email['body_html'] is not None}\")\n",
        "    print(f\"  HTML Length: {len(first_email['body_html']) if first_email['body_html'] else 0} characters\")\n",
        "    print(f\"  Snippet: {first_email['snippet'][:100]}...\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No emails found. Try adjusting search criteria.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Parse HTML Email\n",
        "\n",
        "**What's happening:**\n",
        "- Take raw HTML from email\n",
        "- Remove tracking pixels, scripts, styles\n",
        "- Convert to clean text while preserving structure\n",
        "- Extract all links\n",
        "\n",
        "**Why this matters:**\n",
        "- Newsletters have complex HTML with inline CSS\n",
        "- Tracking pixels and ads add noise\n",
        "- Clean text is better for LLM extraction\n",
        "- Links contain important registration/info URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Email parser initialized\n",
            "\n",
            "üîç Parsing HTML...\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Parsed email: 37643 chars, 51 links\n",
            "\n",
            "‚úÖ Parsing complete!\n",
            "\n",
            "üìÑ Results:\n",
            "  Title: \n",
            "  Clean text length: 37643 characters\n",
            "  Links extracted: 51\n",
            "\n",
            "üîó Sample Links:\n",
            "  1. Read Online... ‚Üí https://elinke1c.daily.therundown.ai/ss/c/u001.dSnm3kaGd0BkN...\n",
            "  2. Read Online... ‚Üí https://elinke1c.daily.therundown.ai/ss/c/u001.dSnm3kaGd0BkN...\n",
            "  3. Sign Up... ‚Üí https://elinke1c.daily.therundown.ai/ss/c/u001.dSnm3kaGd0BkN...\n",
            "\n",
            "üìù Clean Text Preview (first 500 chars):\n",
            "----------------------------------------------------------------------\n",
            "\\---------- Forwarded message --------- \n",
            "From: **The Rundown AI** <[news@daily.therundown.ai](mailto:news@daily.therundown.ai)> \n",
            "Date: Wed, Nov 26, 2025 at 4:45 PM \n",
            "Subject: üîÆ Ilya Sutskever breaks silence on AI's future \n",
            "To: [dishant.ghai@gmail.com](mailto:dishant.ghai@gmail.com) <[dishant.ghai@gmail.com](mailto:dishant.ghai@gmail.com)> \n",
            "\n",
            " \n",
            " \n",
            "\n",
            "__\n",
            "\n",
            "| | | | [ Read Online](https://elinke1c.daily.therundown.ai/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf75St_4h4uVDlb2B1USeB_FItljbHo5xv72nPmD5PN8otD5-4VzDQSeutF2\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Initialize HTML parser\n",
        "# This uses BeautifulSoup4 + html2text\n",
        "parser = EmailParser()\n",
        "\n",
        "print(\"‚úÖ Email parser initialized\")\n",
        "\n",
        "# Parse the first email's HTML\n",
        "if emails and emails[0]['body_html']:\n",
        "    print(\"\\nüîç Parsing HTML...\")\n",
        "    \n",
        "    parsed_result = parser.parse_html_email(emails[0]['body_html'])\n",
        "    \n",
        "    print(f\"\\n‚úÖ Parsing complete!\")\n",
        "    print(f\"\\nüìÑ Results:\")\n",
        "    print(f\"  Title: {parsed_result['title']}\")\n",
        "    print(f\"  Clean text length: {len(parsed_result['text'])} characters\")\n",
        "    print(f\"  Links extracted: {len(parsed_result['links'])}\")\n",
        "    \n",
        "    # Show first few links\n",
        "    print(f\"\\nüîó Sample Links:\")\n",
        "    for i, link in enumerate(parsed_result['links'][:3], 1):\n",
        "        print(f\"  {i}. {link['text'][:50]}... ‚Üí {link['url'][:60]}...\")\n",
        "    \n",
        "    # Show preview of clean text\n",
        "    print(f\"\\nüìù Clean Text Preview (first 500 chars):\")\n",
        "    print(\"-\" * 70)\n",
        "    print(parsed_result['text'][:500])\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    # Store for next step\n",
        "    clean_text = parsed_result['text']\n",
        "    email_subject = emails[0]['subject']\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No HTML body to parse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Extract Events with LLM\n",
        "\n",
        "**What's happening:**\n",
        "- Send clean text to OpenAI GPT-4o-mini\n",
        "- Use structured output (JSON Schema) for guaranteed valid JSON\n",
        "- Extract event details: title, date, location, link, etc.\n",
        "- Get confidence score\n",
        "\n",
        "**How structured output works:**\n",
        "1. We define a Pydantic model (ExtractedEvent)\n",
        "2. Pydantic generates JSON Schema\n",
        "3. OpenAI guarantees response matches schema\n",
        "4. No parsing errors or invalid JSON!\n",
        "\n",
        "**Note:** This step requires OpenAI API key and will make an API call (~$0.0003 per email)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mINFO    \u001b[0m | LLMExtractor initialized: model=gpt-4o-mini\n",
            "‚úÖ LLM extractor initialized\n",
            "   Model: gpt-4o-mini\n",
            "   Temperature: 0.0 (0 = deterministic)\n",
            "\n",
            "ü§ñ Extracting events with LLM...\n",
            "   (This may take 2-4 seconds)\n",
            "\u001b[1mINFO    \u001b[0m | Extracting events from: 'Fwd: üîÆ Ilya Sutskever breaks silence on AI's futur...'\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Extracted 1 events (confidence: 0.90)\n",
            "\n",
            "‚úÖ Extraction complete!\n",
            "\n",
            "üìä Results:\n",
            "  Events found: 1\n",
            "  Confidence: 0.90 (0.0 - 1.0)\n",
            "\n",
            "üéâ Extracted Events:\n",
            "======================================================================\n",
            "\n",
            "Event 1:\n",
            "  üìå Title: Ilya Sutskever breaks silence on AI's future\n",
            "  üìù Description: N/A...\n",
            "  üìÖ Start Time: 2025-11-26T16:45:00\n",
            "  üïí Timezone: Not specified\n",
            "  üìç Location: Online\n",
            "  üîó Registration: https://elinke1c.daily.therundown.ai/ss/c/u001.dSnm3kaGd0BkNqLYPjeMf75St_4h4uVDlb2B1USeB_H4wpkqf8EGTk1iM5Wu0HuoFIsKQVYDu_ovIaAd89V_Nvs2jsW_lRswn84NkqTYMu8JVnw2IsjtESfB6_5xHNXcHSTXHFKTw9U75oBmgjrVqB2SZ9d-NozJirB4eyul8bEm4850lnNbtwLSr25wtfAuEAVwXueEw5LwRnEluHpMLlFQtt4Fwibr0dKzZBmuMqmxo5OKU1yaYUDw4tzKyv8Z2SlRojH7BcZtV6gpBILtJ7m2luOcCaup3Z2_t8s69rXqYATYekt8Dt7Zv8Ru0l-B60kK4_d2D-4cU4f6auy42QtP_-0DoyWzawBddTXb21069Y8VyuCJ37rhiUl08gPhEbaj6fm0HvSd-TbvYDk3uSy6lqIoy2hjlWIMBPLQ4TaUZL7LuNmRJiZwN79vjnhNUdNN2_2jNw_VhLCacQEmtil6v4EJ1SlFMiVpL4RIdjw08KPuFUQmsMczxd_rP4uTU-BHagLKGeayHv3Z6sKE9E5ArLP84sWxyRpP59saWzCzvbDifztttL1nImkTqCwip5mGocLcSS6vNIv4wa8J0Zdg9bhbMYIdSXb2JxRiPP6JMJMv6T3K6fZXNFSInc7xprPdoUzfSa1iZyDvyenrVFkPA1k7Lb4-X0K4qe6kh0Tey02mkoXEOetlHXMRtQZqcjnltuq57zvseujojyR\n",
            "  üí∞ Cost: Free\n",
            "  üè¢ Organizer: The Rundown AI\n",
            "  üè∑Ô∏è  Tags: None\n",
            "  ‚è∞ Status: past\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Initialize LLM extractor\n",
        "# This creates an async OpenAI client\n",
        "extractor = LLMExtractor()\n",
        "\n",
        "print(\"‚úÖ LLM extractor initialized\")\n",
        "print(f\"   Model: {extractor.model}\")\n",
        "print(f\"   Temperature: {extractor.temperature} (0 = deterministic)\")\n",
        "\n",
        "# Extract events from the parsed text\n",
        "# This is an async function, so we need to use await\n",
        "print(\"\\nü§ñ Extracting events with LLM...\")\n",
        "print(\"   (This may take 2-4 seconds)\")\n",
        "\n",
        "# Run async extraction\n",
        "extraction_result = await extractor.extract_events(\n",
        "    email_text=clean_text,\n",
        "    email_subject=email_subject\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Extraction complete!\")\n",
        "print(f\"\\nüìä Results:\")\n",
        "print(f\"  Events found: {len(extraction_result.events)}\")\n",
        "print(f\"  Confidence: {extraction_result.confidence:.2f} (0.0 - 1.0)\")\n",
        "\n",
        "# Display each extracted event\n",
        "if extraction_result.events:\n",
        "    print(f\"\\nüéâ Extracted Events:\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    for i, event in enumerate(extraction_result.events, 1):\n",
        "        print(f\"\\nEvent {i}:\")\n",
        "        print(f\"  üìå Title: {event.title}\")\n",
        "        print(f\"  üìù Description: {event.description[:100] if event.description else 'N/A'}...\")\n",
        "        print(f\"  üìÖ Start Time: {event.start_time or 'Not specified'}\")\n",
        "        print(f\"  üïí Timezone: {event.timezone or 'Not specified'}\")\n",
        "        print(f\"  üìç Location: {event.location or 'Not specified'}\")\n",
        "        print(f\"  üîó Registration: {event.registration_link or 'Not specified'}\")\n",
        "        print(f\"  üí∞ Cost: {event.cost or 'Not specified'}\")\n",
        "        print(f\"  üè¢ Organizer: {event.organizer or 'Not specified'}\")\n",
        "        print(f\"  üè∑Ô∏è  Tags: {', '.join(event.tags) if event.tags else 'None'}\")\n",
        "        print(f\"  ‚è∞ Status: {event.compute_status().value}\")\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No events found in this email\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Generate Embeddings\n",
        "\n",
        "**What's happening:**\n",
        "- Convert event text to dense vector (384 dimensions)\n",
        "- Use sentence-transformers (all-MiniLM-L6-v2)\n",
        "- Vectors capture semantic meaning\n",
        "\n",
        "**Why embeddings:**\n",
        "- Enable semantic search (\"machine learning workshop\" finds \"ML training\")\n",
        "- Similar events have similar vectors\n",
        "- Much better than keyword matching\n",
        "\n",
        "**Model details:**\n",
        "- Runs locally (no API costs)\n",
        "- ~50ms per embedding\n",
        "- First run downloads model (~80MB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Loading embedding model...\n",
            "   (First time: downloads ~80MB model)\n",
            "   (Subsequent: loads from cache)\n",
            "\u001b[1mINFO    \u001b[0m | Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Model loaded: 384D vectors\n",
            "\n",
            "‚úÖ Embedding service initialized\n",
            "   Model: sentence-transformers/all-MiniLM-L6-v2\n",
            "   Dimension: 384D vectors\n",
            "\n",
            "üî¢ Generating embeddings for 1 events...\n",
            "  Event 1: Generated 384D vector\n",
            "\n",
            "üìä Example Vector (first 10 dimensions):\n",
            "   [-0.03619229793548584, 0.002591446042060852, 0.05195854604244232, 0.021957270801067352, 0.042309924960136414, -0.010471072047948837, 0.01925678178668022, 0.0017113657668232918, 0.049980346113443375, -0.028640037402510643]\n",
            "   ... (374 more dimensions)\n"
          ]
        }
      ],
      "source": [
        "# Initialize embedding service\n",
        "# First run will download the model (takes ~30 seconds)\n",
        "# Subsequent runs load from cache (~2 seconds)\n",
        "print(\"üîÑ Loading embedding model...\")\n",
        "print(\"   (First time: downloads ~80MB model)\")\n",
        "print(\"   (Subsequent: loads from cache)\")\n",
        "\n",
        "embedder = EmbeddingService()\n",
        "\n",
        "print(f\"\\n‚úÖ Embedding service initialized\")\n",
        "print(f\"   Model: {embedder.model_name}\")\n",
        "print(f\"   Dimension: {embedder.dimension}D vectors\")\n",
        "\n",
        "# Generate embeddings for each event\n",
        "if extraction_result.events:\n",
        "    print(f\"\\nüî¢ Generating embeddings for {len(extraction_result.events)} events...\")\n",
        "    \n",
        "    event_embeddings = []\n",
        "    \n",
        "    for i, event in enumerate(extraction_result.events, 1):\n",
        "        # Combine title and description for richer embedding\n",
        "        text_to_embed = f\"{event.title} {event.description or ''}\"\n",
        "        \n",
        "        # Generate embedding\n",
        "        vector = embedder.embed_text(text_to_embed)\n",
        "        \n",
        "        event_embeddings.append({\n",
        "            'event': event,\n",
        "            'vector': vector,\n",
        "            'text': text_to_embed\n",
        "        })\n",
        "        \n",
        "        print(f\"  Event {i}: Generated {len(vector)}D vector\")\n",
        "    \n",
        "    # Show example vector (first 10 dimensions)\n",
        "    print(f\"\\nüìä Example Vector (first 10 dimensions):\")\n",
        "    print(f\"   {event_embeddings[0]['vector'][:10]}\")\n",
        "    print(f\"   ... (374 more dimensions)\")\n",
        "    \n",
        "    # Demonstrate semantic similarity\n",
        "    if len(event_embeddings) >= 2:\n",
        "        import numpy as np\n",
        "        \n",
        "        vec1 = np.array(event_embeddings[0]['vector'])\n",
        "        vec2 = np.array(event_embeddings[1]['vector'])\n",
        "        \n",
        "        # Cosine similarity\n",
        "        similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "        \n",
        "        print(f\"\\nüîç Semantic Similarity Example:\")\n",
        "        print(f\"   Event 1: {event_embeddings[0]['event'].title}\")\n",
        "        print(f\"   Event 2: {event_embeddings[1]['event'].title}\")\n",
        "        print(f\"   Similarity: {similarity:.3f} (1.0 = identical, 0.0 = unrelated)\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No events to embed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Store in Qdrant Vector Database\n",
        "\n",
        "**What's happening:**\n",
        "- Connect to Qdrant (running in Docker)\n",
        "- Create collection if doesn't exist\n",
        "- Store vectors + metadata (payload)\n",
        "- Build HNSW index for fast search\n",
        "\n",
        "**Qdrant concepts:**\n",
        "- **Collection**: Container for vectors (like a table)\n",
        "- **Point**: Single vector + payload\n",
        "- **Payload**: Metadata (title, date, location, etc.)\n",
        "- **HNSW**: Fast approximate nearest neighbor search\n",
        "\n",
        "**Why Qdrant:**\n",
        "- Built for vector search\n",
        "- Payload filtering (\"only upcoming events\")\n",
        "- Fast (millisecond queries)\n",
        "- Free and runs locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mINFO    \u001b[0m | Connecting to Qdrant: localhost:6333\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Qdrant client initialized\n",
            "‚úÖ Qdrant client initialized\n",
            "   Host: localhost:6333\n",
            "   Collection: feedprism_emails\n",
            "\n",
            "üóÑÔ∏è  Creating/verifying collection...\n",
            "\u001b[1mINFO    \u001b[0m | Collection already exists: feedprism_emails\n",
            "\n",
            "üìä Collection Info:\n",
            "   Name: feedprism_emails\n",
            "   Points: 4\n",
            "   Status: green\n",
            "\n",
            "üíæ Preparing 1 points for storage...\n",
            "  ‚úì Prepared: Ilya Sutskever breaks silence on AI's future\n",
            "\n",
            "üì§ Upserting points to Qdrant...\n",
            "\u001b[1mINFO    \u001b[0m | Upserting 1 points to feedprism_emails\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Upserted 1 points\n",
            "\n",
            "‚úÖ Successfully stored 1 events!\n",
            "\n",
            "üìä Updated Collection:\n",
            "   Total points: 5\n"
          ]
        }
      ],
      "source": [
        "# Initialize Qdrant client\n",
        "# This connects to Docker container at localhost:6333\n",
        "qdrant = QdrantService()\n",
        "\n",
        "print(\"‚úÖ Qdrant client initialized\")\n",
        "print(f\"   Host: {qdrant.host}:{qdrant.port}\")\n",
        "print(f\"   Collection: {qdrant.collection_name}\")\n",
        "\n",
        "# Create collection (if doesn't exist)\n",
        "print(\"\\nüóÑÔ∏è  Creating/verifying collection...\")\n",
        "qdrant.create_collection(recreate=False)  # Don't delete existing data\n",
        "\n",
        "# Get collection info\n",
        "info = qdrant.get_collection_info()\n",
        "print(f\"\\nüìä Collection Info:\")\n",
        "print(f\"   Name: {info['name']}\")\n",
        "print(f\"   Points: {info['points_count']}\")\n",
        "print(f\"   Status: {info['status']}\")\n",
        "\n",
        "# Prepare points for insertion\n",
        "if event_embeddings:\n",
        "    from qdrant_client.models import PointStruct\n",
        "    \n",
        "    print(f\"\\nüíæ Preparing {len(event_embeddings)} points for storage...\")\n",
        "    \n",
        "    points = []\n",
        "    \n",
        "    for item in event_embeddings:\n",
        "        event = item['event']\n",
        "        vector = item['vector']\n",
        "        \n",
        "        # Create point with unique ID\n",
        "        point = PointStruct(\n",
        "            id=str(uuid.uuid4()),  # Unique identifier\n",
        "            vector=vector,  # 384D embedding\n",
        "            payload={  # Metadata for filtering and display\n",
        "                'title': event.title,\n",
        "                'description': event.description,\n",
        "                'start_time': event.start_time,\n",
        "                'end_time': event.end_time,\n",
        "                'timezone': event.timezone,\n",
        "                'location': event.location,\n",
        "                'registration_link': str(event.registration_link) if event.registration_link else None,\n",
        "                'tags': event.tags,\n",
        "                'organizer': event.organizer,\n",
        "                'cost': event.cost,\n",
        "                'status': event.compute_status().value,\n",
        "                'source_email_id': emails[0]['id'],\n",
        "                'source_subject': emails[0]['subject'],\n",
        "                'source_from': emails[0]['from'],\n",
        "                'type': 'event',\n",
        "                'extracted_at': datetime.now().isoformat()\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        points.append(point)\n",
        "        print(f\"  ‚úì Prepared: {event.title}\")\n",
        "    \n",
        "    # Insert into Qdrant\n",
        "    print(f\"\\nüì§ Upserting points to Qdrant...\")\n",
        "    qdrant.upsert_points(points)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Successfully stored {len(points)} events!\")\n",
        "    \n",
        "    # Get updated collection info\n",
        "    updated_info = qdrant.get_collection_info()\n",
        "    print(f\"\\nüìä Updated Collection:\")\n",
        "    print(f\"   Total points: {updated_info['points_count']}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No events to store\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Semantic Search\n",
        "\n",
        "**What's happening:**\n",
        "- Convert search query to vector\n",
        "- Find similar vectors in Qdrant\n",
        "- Return events ranked by similarity\n",
        "- Apply filters (type, status, etc.)\n",
        "\n",
        "**How it works:**\n",
        "1. Query: \"machine learning workshop\" ‚Üí vector\n",
        "2. Qdrant finds nearest neighbor vectors\n",
        "3. Returns events with similarity scores\n",
        "4. Higher score = more relevant\n",
        "\n",
        "**Magic of semantic search:**\n",
        "- \"ML training\" matches \"machine learning workshop\"\n",
        "- \"AI conference\" matches \"artificial intelligence summit\"\n",
        "- Works across synonyms and related concepts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Semantic Search Demo\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üîé Query: 'machine learning workshop'\n",
            "----------------------------------------------------------------------\n",
            "\u001b[1mINFO    \u001b[0m | Searching feedprism_emails (limit=3)\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Found 3 results\n",
            "Found 3 results:\n",
            "\n",
            "1. AI Conference on Machine Learning\n",
            "   üìä Relevance: 0.715 (0.0 - 1.0)\n",
            "   üìç Location: N/A\n",
            "   üìÖ Start: N/A\n",
            "   ‚è∞ Status: N/A\n",
            "   üîó Link: N/A...\n",
            "\n",
            "2. Python Workshop for Beginners\n",
            "   üìä Relevance: 0.560 (0.0 - 1.0)\n",
            "   üìç Location: N/A\n",
            "   üìÖ Start: N/A\n",
            "   ‚è∞ Status: N/A\n",
            "   üîó Link: N/A...\n",
            "\n",
            "3. Engineering Machine tools Learning Event\n",
            "   üìä Relevance: 0.535 (0.0 - 1.0)\n",
            "   üìç Location: N/A\n",
            "   üìÖ Start: N/A\n",
            "   ‚è∞ Status: N/A\n",
            "   üîó Link: N/A...\n",
            "\n",
            "\n",
            "üîé Query: 'AI conference'\n",
            "----------------------------------------------------------------------\n",
            "\u001b[1mINFO    \u001b[0m | Searching feedprism_emails (limit=3)\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Found 3 results\n",
            "Found 3 results:\n",
            "\n",
            "1. AI Conference on Machine Learning\n",
            "   üìä Relevance: 0.853 (0.0 - 1.0)\n",
            "   üìç Location: N/A\n",
            "   üìÖ Start: N/A\n",
            "   ‚è∞ Status: N/A\n",
            "   üîó Link: N/A...\n",
            "\n",
            "2. Data Science Meetup\n",
            "   üìä Relevance: 0.399 (0.0 - 1.0)\n",
            "   üìç Location: N/A\n",
            "   üìÖ Start: N/A\n",
            "   ‚è∞ Status: N/A\n",
            "   üîó Link: N/A...\n",
            "\n",
            "3. Engineering Machine tools Learning Event\n",
            "   üìä Relevance: 0.396 (0.0 - 1.0)\n",
            "   üìç Location: N/A\n",
            "   üìÖ Start: N/A\n",
            "   ‚è∞ Status: N/A\n",
            "   üîó Link: N/A...\n",
            "\n",
            "\n",
            "üîé Query: 'data science event'\n",
            "----------------------------------------------------------------------\n",
            "\u001b[1mINFO    \u001b[0m | Searching feedprism_emails (limit=3)\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Found 3 results\n",
            "Found 3 results:\n",
            "\n",
            "1. Data Science Meetup\n",
            "   üìä Relevance: 0.681 (0.0 - 1.0)\n",
            "   üìç Location: N/A\n",
            "   üìÖ Start: N/A\n",
            "   ‚è∞ Status: N/A\n",
            "   üîó Link: N/A...\n",
            "\n",
            "2. Engineering Machine tools Learning Event\n",
            "   üìä Relevance: 0.617 (0.0 - 1.0)\n",
            "   üìç Location: N/A\n",
            "   üìÖ Start: N/A\n",
            "   ‚è∞ Status: N/A\n",
            "   üîó Link: N/A...\n",
            "\n",
            "3. AI Conference on Machine Learning\n",
            "   üìä Relevance: 0.421 (0.0 - 1.0)\n",
            "   üìç Location: N/A\n",
            "   üìÖ Start: N/A\n",
            "   ‚è∞ Status: N/A\n",
            "   üîó Link: N/A...\n",
            "\n",
            "\n",
            "üîé Query: 'online webinar'\n",
            "----------------------------------------------------------------------\n",
            "\u001b[1mINFO    \u001b[0m | Searching feedprism_emails (limit=3)\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Found 3 results\n",
            "Found 3 results:\n",
            "\n",
            "1. Data Science Meetup\n",
            "   üìä Relevance: 0.258 (0.0 - 1.0)\n",
            "   üìç Location: N/A\n",
            "   üìÖ Start: N/A\n",
            "   ‚è∞ Status: N/A\n",
            "   üîó Link: N/A...\n",
            "\n",
            "2. Python Workshop for Beginners\n",
            "   üìä Relevance: 0.202 (0.0 - 1.0)\n",
            "   üìç Location: N/A\n",
            "   üìÖ Start: N/A\n",
            "   ‚è∞ Status: N/A\n",
            "   üîó Link: N/A...\n",
            "\n",
            "3. AI Conference on Machine Learning\n",
            "   üìä Relevance: 0.135 (0.0 - 1.0)\n",
            "   üìç Location: N/A\n",
            "   üìÖ Start: N/A\n",
            "   ‚è∞ Status: N/A\n",
            "   üîó Link: N/A...\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Let's try some searches!\n",
        "print(\"üîç Semantic Search Demo\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Define search queries\n",
        "search_queries = [\n",
        "    \"machine learning workshop\",\n",
        "    \"AI conference\",\n",
        "    \"data science event\",\n",
        "    \"online webinar\"\n",
        "]\n",
        "\n",
        "for query in search_queries:\n",
        "    print(f\"\\nüîé Query: '{query}'\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    # Generate query vector\n",
        "    query_vector = embedder.embed_text(query)\n",
        "    \n",
        "    # Search Qdrant\n",
        "    # Parameters:\n",
        "    #   query_vector: The embedding of our search query\n",
        "    #   limit: How many results to return\n",
        "    #   filter_dict: Optional filters (e.g., only upcoming events)\n",
        "    results = qdrant.search(\n",
        "        query_vector=query_vector,\n",
        "        limit=3,\n",
        "        filter_dict={'type': 'event'}  # Only return events\n",
        "    )\n",
        "    \n",
        "    if results:\n",
        "        print(f\"Found {len(results)} results:\\n\")\n",
        "        \n",
        "        for i, result in enumerate(results, 1):\n",
        "            payload = result['payload']\n",
        "            score = result['score']\n",
        "            \n",
        "            print(f\"{i}. {payload['title']}\")\n",
        "            print(f\"   üìä Relevance: {score:.3f} (0.0 - 1.0)\")\n",
        "            print(f\"   üìç Location: {payload.get('location', 'N/A')}\")\n",
        "            print(f\"   üìÖ Start: {payload.get('start_time', 'N/A')}\")\n",
        "            print(f\"   ‚è∞ Status: {payload.get('status', 'N/A')}\")\n",
        "            print(f\"   üîó Link: {payload.get('registration_link', 'N/A')[:60]}...\")\n",
        "            print()\n",
        "    else:\n",
        "        print(\"No results found\\n\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Advanced Search with Filters\n",
        "\n",
        "**Payload filtering:**\n",
        "- Search within specific criteria\n",
        "- Combine semantic search + filters\n",
        "- Much more powerful than search alone\n",
        "\n",
        "**Examples:**\n",
        "- \"ML events\" + only upcoming\n",
        "- \"conferences\" + only free\n",
        "- \"workshops\" + only online"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Filtered Search Demo\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üîé Search: 'AI event' (upcoming only)\n",
            "----------------------------------------------------------------------\n",
            "\u001b[1mINFO    \u001b[0m | Searching feedprism_emails (limit=5)\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Found 0 results\n",
            "Found 0 upcoming events\n",
            "\n",
            "\n",
            "üîé Search: 'workshop' (online only)\n",
            "----------------------------------------------------------------------\n",
            "\u001b[1mINFO    \u001b[0m | Searching feedprism_emails (limit=5)\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Found 1 results\n",
            "Found 1 online workshops\n",
            "\n",
            "1. Ilya Sutskever breaks silence on AI's future (score: 0.078)\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"üéØ Filtered Search Demo\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Search 1: Only upcoming events\n",
        "print(\"\\nüîé Search: 'AI event' (upcoming only)\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "query_vec = embedder.embed_text(\"AI event\")\n",
        "results = qdrant.search(\n",
        "    query_vector=query_vec,\n",
        "    limit=5,\n",
        "    filter_dict={\n",
        "        'type': 'event',\n",
        "        'status': 'upcoming'  # Only future events\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Found {len(results)} upcoming events\\n\")\n",
        "for i, r in enumerate(results, 1):\n",
        "    print(f\"{i}. {r['payload']['title']} (score: {r['score']:.3f})\")\n",
        "\n",
        "# Search 2: Only online events\n",
        "print(\"\\nüîé Search: 'workshop' (online only)\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "query_vec = embedder.embed_text(\"workshop\")\n",
        "results = qdrant.search(\n",
        "    query_vector=query_vec,\n",
        "    limit=5,\n",
        "    filter_dict={\n",
        "        'type': 'event',\n",
        "        'location': 'Online'  # Only virtual events\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Found {len(results)} online workshops\\n\")\n",
        "for i, r in enumerate(results, 1):\n",
        "    print(f\"{i}. {r['payload']['title']} (score: {r['score']:.3f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Complete Pipeline\n",
        "\n",
        "**What we accomplished:**\n",
        "\n",
        "1. ‚úÖ **Fetched emails** from Gmail API\n",
        "2. ‚úÖ **Parsed HTML** to clean text\n",
        "3. ‚úÖ **Extracted events** using LLM structured output\n",
        "4. ‚úÖ **Generated embeddings** for semantic search\n",
        "5. ‚úÖ **Stored in Qdrant** with rich metadata\n",
        "6. ‚úÖ **Searched semantically** with filters\n",
        "\n",
        "**Key learnings:**\n",
        "\n",
        "- **Gmail API**: OAuth authentication, multipart emails\n",
        "- **HTML Parsing**: BeautifulSoup4, tracking pixel removal\n",
        "- **LLM Extraction**: Structured output guarantees valid JSON\n",
        "- **Embeddings**: 384D vectors capture semantic meaning\n",
        "- **Qdrant**: Vector database with payload filtering\n",
        "- **Semantic Search**: Find by meaning, not just keywords\n",
        "\n",
        "**Performance:**\n",
        "\n",
        "- Email fetch: ~3s for 5 emails\n",
        "- HTML parsing: ~50ms per email\n",
        "- Event extraction: ~2-3s per email\n",
        "- Embedding: ~50ms per event\n",
        "- Qdrant storage: ~200ms for batch\n",
        "- Search: ~100ms per query\n",
        "\n",
        "**Next steps:**\n",
        "\n",
        "- **Phase 2**: Add courses and blogs extraction\n",
        "- **Phase 3**: Implement hybrid search (dense + sparse)\n",
        "- **Phase 4**: Advanced Qdrant features (grouping, discovery)\n",
        "- **Phase 5**: Deduplication and temporal reasoning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus: Process Multiple Emails\n",
        "\n",
        "Now that you understand each step, let's process all fetched emails in a loop!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Processing All Emails\n",
            "\n",
            "======================================================================\n",
            "\n",
            "[1/5] Processing: Fwd: üîÆ Ilya Sutskever breaks silence on AI's future...\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Parsed email: 37643 chars, 51 links\n",
            "  ‚úì Parsed: 37643 chars, 51 links\n",
            "\u001b[1mINFO    \u001b[0m | Extracting events from: 'Fwd: üîÆ Ilya Sutskever breaks silence on AI's futur...'\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Extracted 1 events (confidence: 0.90)\n",
            "  ‚úì Extracted: 1 events (confidence: 0.90)\n",
            "  ‚úì Created 1 points\n",
            "\n",
            "[2/5] Processing: RBI Retail Direct - New RBI Retail Direct Portal...\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Parsed email: 337 chars, 0 links\n",
            "  ‚úì Parsed: 337 chars, 0 links\n",
            "\u001b[1mINFO    \u001b[0m | Extracting events from: 'RBI Retail Direct - New RBI Retail Direct Portal...'\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Extracted 1 events (confidence: 0.90)\n",
            "  ‚úì Extracted: 1 events (confidence: 0.90)\n",
            "  ‚úì Created 1 points\n",
            "\n",
            "[3/5] Processing: I made my resume site. It wasn‚Äôt that deep. | DEV Digest...\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Parsed email: 4601 chars, 11 links\n",
            "  ‚úì Parsed: 4601 chars, 11 links\n",
            "\u001b[1mINFO    \u001b[0m | Extracting events from: 'I made my resume site. It wasn‚Äôt that deep. | DEV ...'\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Extracted 0 events (confidence: 0.00)\n",
            "  ‚úì Extracted: 0 events (confidence: 0.00)\n",
            "\n",
            "[4/5] Processing: I want to connect...\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Parsed email: 22709 chars, 41 links\n",
            "  ‚úì Parsed: 22709 chars, 41 links\n",
            "\u001b[1mINFO    \u001b[0m | Extracting events from: 'I want to connect...'\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Extracted 0 events (confidence: 0.00)\n",
            "  ‚úì Extracted: 0 events (confidence: 0.00)\n",
            "\n",
            "[5/5] Processing: üîÆ Ilya Sutskever breaks silence on AI's future...\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Parsed email: 37291 chars, 51 links\n",
            "  ‚úì Parsed: 37291 chars, 51 links\n",
            "\u001b[1mINFO    \u001b[0m | Extracting events from: 'üîÆ Ilya Sutskever breaks silence on AI's future...'\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Extracted 1 events (confidence: 0.90)\n",
            "  ‚úì Extracted: 1 events (confidence: 0.90)\n",
            "  ‚úì Created 1 points\n",
            "\n",
            "üíæ Storing 3 total events...\n",
            "\u001b[1mINFO    \u001b[0m | Upserting 3 points to feedprism_emails\n",
            "\u001b[32m\u001b[1mSUCCESS \u001b[0m | Upserted 3 points\n",
            "‚úÖ Complete!\n",
            "\n",
            "======================================================================\n",
            "üìä Final Statistics:\n",
            "   Emails processed: 5\n",
            "   Events extracted: 3\n",
            "   Points stored: 3\n",
            "   Total in database: 8 points\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"üîÑ Processing All Emails\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "total_events = 0\n",
        "all_points = []\n",
        "\n",
        "for i, email in enumerate(emails, 1):\n",
        "    print(f\"\\n[{i}/{len(emails)}] Processing: {email['subject'][:60]}...\")\n",
        "    \n",
        "    try:\n",
        "        # Skip if no HTML\n",
        "        if not email['body_html']:\n",
        "            print(\"  ‚ö†Ô∏è No HTML body, skipping\")\n",
        "            continue\n",
        "        \n",
        "        # Parse\n",
        "        parsed = parser.parse_html_email(email['body_html'])\n",
        "        print(f\"  ‚úì Parsed: {len(parsed['text'])} chars, {len(parsed['links'])} links\")\n",
        "        \n",
        "        # Extract\n",
        "        result = await extractor.extract_events(parsed['text'], email['subject'])\n",
        "        print(f\"  ‚úì Extracted: {len(result.events)} events (confidence: {result.confidence:.2f})\")\n",
        "        \n",
        "        if not result.events:\n",
        "            continue\n",
        "        \n",
        "        # Embed and create points\n",
        "        for event in result.events:\n",
        "            text = f\"{event.title} {event.description or ''}\"\n",
        "            vector = embedder.embed_text(text)\n",
        "            \n",
        "            from qdrant_client.models import PointStruct\n",
        "            point = PointStruct(\n",
        "                id=str(uuid.uuid4()),\n",
        "                vector=vector,\n",
        "                payload={\n",
        "                    'title': event.title,\n",
        "                    'description': event.description,\n",
        "                    'start_time': event.start_time,\n",
        "                    'location': event.location,\n",
        "                    'registration_link': str(event.registration_link) if event.registration_link else None,\n",
        "                    'tags': event.tags,\n",
        "                    'status': event.compute_status().value,\n",
        "                    'source_email_id': email['id'],\n",
        "                    'source_subject': email['subject'],\n",
        "                    'type': 'event'\n",
        "                }\n",
        "            )\n",
        "            all_points.append(point)\n",
        "        \n",
        "        total_events += len(result.events)\n",
        "        print(f\"  ‚úì Created {len(result.events)} points\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error: {e}\")\n",
        "        continue\n",
        "\n",
        "# Store all points\n",
        "if all_points:\n",
        "    print(f\"\\nüíæ Storing {len(all_points)} total events...\")\n",
        "    qdrant.upsert_points(all_points)\n",
        "    print(f\"‚úÖ Complete!\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 70)\n",
        "print(f\"üìä Final Statistics:\")\n",
        "print(f\"   Emails processed: {len(emails)}\")\n",
        "print(f\"   Events extracted: {total_events}\")\n",
        "print(f\"   Points stored: {len(all_points)}\")\n",
        "\n",
        "# Final collection info\n",
        "final_info = qdrant.get_collection_info()\n",
        "print(f\"   Total in database: {final_info['points_count']} points\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
