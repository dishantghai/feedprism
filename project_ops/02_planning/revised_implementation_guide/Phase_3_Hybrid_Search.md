# Phase 3: Hybrid Search - Dense + Sparse Vectors with Filtering

**Goal:** Add hybrid search (dense + sparse BM25), payload filtering, and time-aware queries for improved retrieval quality.

**Estimated Time:** 6-8 hours

**Prerequisites:** Phase 2 complete (verified ‚úÖ)

---

## Overview

### What We're Adding

**Current (Dense-only):**
```
Query ‚Üí Embed ‚Üí Dense Vector Search ‚Üí Results
```

**Enhanced (Hybrid):**
```
Query ‚Üí Embed (Dense) ‚Üí ‚îê
Query ‚Üí Tokenize (Sparse) ‚Üí ‚îú‚Üí RRF Fusion ‚Üí Filter ‚Üí Results
                             ‚îò
```

**Key Improvements:**
1. **Sparse Vectors (BM25-style):** Keyword matching (complements semantic search)
2. **Reciprocal Rank Fusion (RRF):** Combine dense + sparse scores
3. **Payload Filtering:** Pre-filter by content_type, date, tags
4. **Time-aware Queries:** Upcoming events, recent articles

---

## Modules

### Module 3.1: Enable Sparse Vectors in Qdrant (2 hours)

**Location:** `feedprism_main/app/database/qdrant_client.py`

**Update collection configuration:**

The `create_collection` method must configure both dense and sparse vector support:

```python
# In qdrant_client.py create_collection method
from qdrant_client.models import VectorParams, SparseVectorParams, Distance

def create_collection(self, recreate: bool = False) -> None:
    """Create or recreate the Qdrant collection with hybrid vector support."""
    
    if recreate:
        try:
            self.client.delete_collection(collection_name=self.collection_name)
            logger.warning(f"Deleted existing collection: {self.collection_name}")
        except Exception:
            pass
    
    logger.info(f"Creating collection: {self.collection_name}")
    
    self.client.create_collection(
        collection_name=self.collection_name,
        vectors_config=VectorParams(
            size=self.vector_size,  # 384 for all-MiniLM-L6-v2
            distance=Distance.COSINE
        ),
        sparse_vectors_config={
            "keywords": SparseVectorParams()  # Named sparse vector field
        }
    )
    
    logger.success(f"Collection created: {self.collection_name}")
```

**Key points:**
- Dense vectors use the default (unnamed) vector field
- Sparse vectors use a named field `"keywords"`
- Both vector types are indexed automatically by Qdrant

---

### Module 3.2: Sparse Vector Generation (1.5 hours)

**Location:** `feedprism_main/app/utils/sparse_vector.py` (new file)

**Implementation:**

Create a BM25-style sparse vector generator using term frequency and consistent hashing:

```python
"""
Utility to generate a simple BM25-style sparse vector from raw text.
"""
import re
from collections import Counter
from typing import Dict, List, Any

def create_sparse_vector(text: str) -> Dict[str, List[Any]]:
    """
    Generate sparse vector from text (simple keyword extraction).
    
    Returns a dict with `indices` and `values` suitable for Qdrant's
    sparse vector API.
    
    The implementation uses a lightweight TF (term-frequency) extraction
    with consistent hashing to map words to indices.
    
    For production, you may replace with proper BM25, SPLADE, or other
    lexical weighting schemes.
    
    Args:
        text: Input text to vectorize
        
    Returns:
        Dict with 'indices' (List[int]) and 'values' (List[float])
    """
    # Tokenize - keep only word characters, lower-cased
    words = re.findall(r'\b\w+\b', text.lower())
    
    # Count frequencies (simple TF)
    word_counts = Counter(words)
    
    # Create sparse vector using hashing trick
    # This ensures consistent indices for the same words across documents
    indices = []
    values = []
    
    for word, count in word_counts.items():
        # Simple hashing to get a consistent index for each word
        # Use modulo with a large space to minimize collisions (e.g., 1M dimensions)
        idx = hash(word) % 1000000
        indices.append(idx)
        values.append(float(count))
        
    return {"indices": indices, "values": values}
```

**Why hashing?**
- Ensures the same word always maps to the same index
- No need to maintain a global vocabulary
- Collisions are rare with a large hash space (1M dimensions)
- Standard "hashing trick" in ML for sparse features

**Testing:**
```python
# Test sparse vector generation
from app.utils.sparse_vector import create_sparse_vector

text1 = "Machine learning workshop on deep learning"
text2 = "Python programming course"

sparse1 = create_sparse_vector(text1)
sparse2 = create_sparse_vector(text2)

print(f"Text 1 has {len(sparse1['indices'])} unique words")
print(f"'learning' appears {sparse1['values'][sparse1['indices'].index(hash('learning') % 1000000)]} times")
```

---

### Module 3.3: Hybrid Search Implementation (2.5 hours)

**Location:** `feedprism_main/app/database/qdrant_client.py`

**Add hybrid search method:**

```python
from app.utils.sparse_vector import create_sparse_vector
from qdrant_client.models import SparseVector, NamedSparseVector

def hybrid_search(
    self,
    query_vector: List[float],
    query_text: str,
    limit: int = 10
) -> List[Dict]:
    """
    Hybrid search: dense + sparse with RRF fusion.
    
    Combines semantic search (dense vectors) with keyword matching (sparse vectors)
    using Reciprocal Rank Fusion for optimal results.
    
    Args:
        query_vector: Dense embedding of the query
        query_text: Raw query text for sparse vector generation
        limit: Number of results to return
        
    Returns:
        List of dicts with 'id', 'score', and 'payload' keys
    """
    # 1. Dense search - retrieve 2x limit for better fusion
    dense_results = self.client.search(
        collection_name=self.collection_name,
        query_vector=query_vector,
        limit=limit * 2
    )
    
    # 2. Sparse search - convert text to sparse vector
    sparse_vec_dict = create_sparse_vector(query_text)
    sparse_vec = SparseVector(**sparse_vec_dict)
    sparse_results = self.client.search(
        collection_name=self.collection_name,
        query_vector=NamedSparseVector(name="keywords", vector=sparse_vec),
        limit=limit * 2
    )
    
    # 3. RRF Fusion - combine rankings
    fused_ids = self._rrf_fusion(dense_results, sparse_results, k=60)
    
    # 4. Retrieve full payloads for top results
    top_ids = fused_ids[:limit]
    if not top_ids:
        return []
        
    points = self.client.retrieve(
        collection_name=self.collection_name,
        ids=top_ids
    )
    
    # 5. Maintain RRF ranking order
    point_map = {point.id: point for point in points}
    ordered_results = []
    for pid in top_ids:
        if pid in point_map:
            point = point_map[pid]
            ordered_results.append({
                "id": point.id,
                "score": 0.0,  # RRF uses ranks, not probability scores
                "payload": point.payload
            })
            
    return ordered_results

def _rrf_fusion(self, dense_results, sparse_results, k=60):
    """
    Reciprocal Rank Fusion algorithm.
    
    Combines two result sets by summing reciprocal ranks:
    score(doc) = Œ£ 1/(k + rank_i)
    
    where k=60 is a constant (standard in literature)
    and rank_i is the position in each result list.
    
    Args:
        dense_results: Results from dense vector search
        sparse_results: Results from sparse vector search
        k: RRF constant (default: 60)
        
    Returns:
        List of document IDs sorted by combined score (descending)
    """
    scores = {}
    
    # Aggregate scores from dense search
    for rank, result in enumerate(dense_results, 1):
        scores[result.id] = scores.get(result.id, 0) + 1 / (k + rank)
    
    # Aggregate scores from sparse search
    for rank, result in enumerate(sparse_results, 1):
        scores[result.id] = scores.get(result.id, 0) + 1 / (k + rank)
    
    # Sort by combined score (descending)
    sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)
    
    return sorted_ids
```

**Why RRF?**
- Doesn't require score normalization (dense and sparse use different scales)
- Robust to outliers
- Simple and effective
- Well-studied in information retrieval literature

---

### Module 3.4: Payload Filtering (2 hours)

**Location:** `feedprism_main/app/database/qdrant_client.py`

**Add filter support:**

```python
from qdrant_client.models import Filter, FieldCondition, MatchValue, Range
from datetime import datetime
from typing import Optional, List, Dict, Tuple

def search_with_filters(
    self,
    query_vector: List[float],
    content_type: Optional[str] = None,
    date_range: Optional[Tuple[str, str]] = None,  # ISO date strings
    tags: Optional[List[str]] = None,
    limit: int = 10
) -> List[Dict]:
    """
    Search with payload filters.
    
    Allows filtering results by content type, date range, and tags
    before or during semantic search.
    
    Args:
        query_vector: Dense embedding of the query
        content_type: Filter by 'event', 'course', 'blog', etc.
        date_range: Tuple of (start_date, end_date) as ISO strings
        tags: List of tags to filter by (all must match)
        limit: Number of results to return
        
    Returns:
        List of filtered search results
    """
    must_conditions = []
    
    # Filter by content type
    if content_type:
        must_conditions.append(
            FieldCondition(
                key="content_type",
                match=MatchValue(value=content_type)
            )
        )
    
    # Filter by date range (convert ISO strings to timestamps)
    if date_range:
        start_date, end_date = date_range
        # Convert ISO strings to Unix timestamps for Range filter
        if isinstance(start_date, str):
            start_date = datetime.fromisoformat(start_date).timestamp()
        if isinstance(end_date, str):
            end_date = datetime.fromisoformat(end_date).timestamp()
            
        must_conditions.append(
            FieldCondition(
                key="start_date",
                range=Range(gte=start_date, lte=end_date)
            )
        )
    
    # Filter by tags (all must match)
    if tags:
        for tag in tags:
            must_conditions.append(
                FieldCondition(
                    key="tags",
                    match=MatchValue(value=tag)
                )
            )
    
    # Create filter object
    query_filter = Filter(must=must_conditions) if must_conditions else None
    
    # Execute filtered search
    results = self.client.search(
        collection_name=self.collection_name,
        query_vector=query_vector,
        query_filter=query_filter,
        limit=limit
    )
    
    return [
        {
            "id": r.id,
            "score": r.score,
            "payload": r.payload
        }
        for r in results
    ]
```

**Important: Date Storage**

Qdrant's `Range` filter requires **numeric values**. Store dates as Unix timestamps (floats):

```python
# When upserting points
from datetime import datetime

event_date = "2025-12-25T10:00:00"
timestamp = datetime.fromisoformat(event_date).timestamp()

point = PointStruct(
    id=1,
    vector={"": dense_vec, "keywords": sparse_vec},
    payload={
        "title": "Christmas ML Workshop",
        "start_date": timestamp,  # Numeric for filtering
        "start_date_iso": event_date,  # String for display
        "content_type": "event"
    }
)
```

---

### Module 3.5: Time-Aware Queries (1.5 hours)

**Location:** `feedprism_main/app/database/qdrant_client.py`

**Add helper methods:**

```python
from datetime import datetime, timedelta

def search_upcoming_events(
    self,
    query_vector: List[float],
    days_ahead: int = 30,
    limit: int = 10
) -> List[Dict]:
    """
    Search for upcoming events only.
    
    Filters events occurring within the next N days.
    
    Args:
        query_vector: Dense embedding of the query
        days_ahead: Number of days to look ahead (default: 30)
        limit: Number of results to return
        
    Returns:
        List of upcoming event results
    """
    today = datetime.now().isoformat()
    future = (datetime.now() + timedelta(days=days_ahead)).isoformat()
    
    return self.search_with_filters(
        query_vector=query_vector,
        content_type="event",
        date_range=(today, future),
        limit=limit
    )

def search_recent_blogs(
    self,
    query_vector: List[float],
    days_back: int = 7,
    limit: int = 10
) -> List[Dict]:
    """
    Search for recent blog posts.
    
    Filters blog posts published within the last N days.
    
    Args:
        query_vector: Dense embedding of the query
        days_back: Number of days to look back (default: 7)
        limit: Number of results to return
        
    Returns:
        List of recent blog post results
    """
    past = (datetime.now() - timedelta(days=days_back)).isoformat()
    today = datetime.now().isoformat()
    
    return self.search_with_filters(
        query_vector=query_vector,
        content_type="blog",
        date_range=(past, today),
        limit=limit
    )
```

---

## Verification

### Automated Verification Script

**Location:** `feedprism_main/scripts/verification/verify_phase_3.py`

Run comprehensive search quality tests:

```bash
cd /Users/Shared/ALL\ WORKSPACE/Hackathons/mom_hack/feedprism_main
python scripts/verification/verify_phase_3.py
```

**What it tests:**
1. ‚úÖ Hybrid search (dense + sparse + RRF)
2. ‚úÖ Content type filtering
3. ‚úÖ Date range filtering (upcoming events)
4. ‚úÖ Date range filtering (recent blogs)
5. ‚úÖ Tag filtering

### Manual Testing

```python
from app.database.qdrant_client import QdrantService
from app.services.embedder import EmbeddingService

qdrant = QdrantService()
embedder = EmbeddingService()

query = "machine learning workshop"
query_vec = embedder.embed_text(query)

# Test 1: Dense-only search (baseline)
dense_results = qdrant.search(query_vec, limit=5)
print(f"Dense-only: {len(dense_results)} results")
for r in dense_results:
    print(f"  - {r['payload']['title']} (score: {r['score']:.3f})")

# Test 2: Hybrid search (Phase 3)
hybrid_results = qdrant.hybrid_search(query_vec, query, limit=5)
print(f"\nHybrid: {len(hybrid_results)} results")
for r in hybrid_results:
    print(f"  - {r['payload']['title']}")

# Test 3: Filtered search (events only)
filtered = qdrant.search_upcoming_events(query_vec, days_ahead=30, limit=5)
print(f"\nUpcoming events: {len(filtered)} results")
for r in filtered:
    date = r['payload'].get('start_date_iso', 'N/A')
    print(f"  - [{date}] {r['payload']['title']}")

print("\n‚úÖ All search modes working!")
```

---

## Educational Resources

### Phase 3 Educational Walkthrough

**Location:** `feedprism_main/notebooks/phase3_educational_walkthrough.ipynb`

An interactive Jupyter notebook that explains:
- Dense vs. sparse vectors (conceptual)
- Sparse vector generation (hands-on example)
- Hybrid search with RRF fusion (step-by-step)
- Payload filtering (content type, date, tags)
- Time-aware queries (upcoming/recent)
- Side-by-side comparison of all search modes

**To run:**
```bash
cd feedprism_main/notebooks
jupyter notebook phase3_educational_walkthrough.ipynb
```

### Phase 3 Evaluation Notebook

**Location:** `feedprism_main/notebooks/phase3_evaluation.ipynb`

Quantitative evaluation comparing Dense-only vs. Hybrid search:
- **Metrics:** Precision@3, Recall@3, MRR, Top-1 Accuracy, Latency
- **Test data:** 20 documents, 10 queries with ground truth
- **Results:** Side-by-side comparison with improvement percentages

**Expected improvements:**
- Precision: +20-25%
- Top-1 Accuracy: +30-40%
- Latency: +10-15ms overhead

---

## Implementation Checklist

- [ ] Module 3.1: Update `create_collection()` with sparse vector config
- [ ] Module 3.2: Create `app/utils/sparse_vector.py`
- [ ] Module 3.3: Add `hybrid_search()` and `_rrf_fusion()` methods
- [ ] Module 3.4: Add `search_with_filters()` method
- [ ] Module 3.5: Add `search_upcoming_events()` and `search_recent_blogs()`
- [ ] Verification: Run `verify_phase_3.py` ‚úÖ
- [ ] Documentation: Review educational walkthrough notebook
- [ ] Evaluation: Run evaluation notebook and review metrics
- [ ] Git Commit: Commit all changes with descriptive message

---

## Git Commit

```bash
cd /Users/Shared/ALL\ WORKSPACE/Hackathons/mom_hack
git add feedprism_main/
git commit -m "feat(feedprism): Phase 3 complete - Hybrid search with filtering

- Sparse vectors (BM25-style) for keyword matching via hashing trick
- RRF fusion for dense + sparse combination (k=60)
- Payload filtering (type, date range with timestamps, tags)
- Time-aware query helpers (upcoming events, recent blogs)
- Date fields stored as Unix timestamps for Range filtering
- Educational walkthrough notebook explaining concepts
- Evaluation notebook showing 20-40% improvement metrics

Search quality significantly improved through hybrid approach"

git tag feedprism-phase-3-complete
```

---

## Troubleshooting

### Common Issues

**Issue 1: ValidationError on Range filter**
```
qdrant_client.http.exceptions.ResponseHandlingException: 
Validation error in JSON body: [start_date.range.gte: invalid type: string "2025-11-27...", expected f64]
```

**Solution:** Dates must be stored as numeric timestamps (floats), not ISO strings.

```python
# ‚úÖ Correct
start_date = datetime.fromisoformat("2025-11-27").timestamp()  # float

# ‚ùå Wrong
start_date = "2025-11-27"  # string
```

---

**Issue 2: Sparse search returns no results**

**Solution:** Ensure sparse vectors are properly formatted:

```python
# ‚úÖ Correct
sparse_vec_dict = create_sparse_vector(query_text)  # {"indices": [...], "values": [...]}
sparse_vec = SparseVector(**sparse_vec_dict)
query_vector=NamedSparseVector(name="keywords", vector=sparse_vec)

# ‚ùå Wrong
query_vector=("keywords", sparse_vec_dict)  # Dict instead of SparseVector
```

---

**Issue 3: RRF fusion returns fewer results than expected**

**Solution:** Fetch more candidates (2x limit) from each search before fusion:

```python
# ‚úÖ Correct
dense_results = self.client.search(..., limit=limit * 2)
sparse_results = self.client.search(..., limit=limit * 2)
fused_ids = self._rrf_fusion(...)[:limit]  # Take top N after fusion

# ‚ùå Wrong
dense_results = self.client.search(..., limit=limit)  # Not enough candidates
```

---

## Performance Considerations

### Latency

- **Dense-only:** ~10-15ms per query
- **Hybrid:** ~20-25ms per query (+50% overhead)
- **Trade-off:** 10ms for 20-40% better relevance

### Memory

- **Sparse vectors:** Minimal overhead (~1-5 KB per document)
- **Hash space:** 1M dimensions uses ~4 MB for index metadata
- **Total:** Negligible for <100K documents

### Scalability

For >100K documents:
- Consider quantization (Phase 4)
- HNSW tuning for faster search
- Sharding across multiple nodes

---

## Phase 3 Complete! üéâ

**What we built:**
- ‚úÖ Dual-vector storage (dense + sparse)
- ‚úÖ Hybrid search with RRF fusion
- ‚úÖ Payload filtering (type, date, tags)
- ‚úÖ Time-aware query helpers
- ‚úÖ Educational walkthrough notebook
- ‚úÖ Quantitative evaluation metrics

**Next Step:** **[Phase 4: Qdrant Enhancements](Phase_4_Qdrant_Enhancements.md)**
- Quantization for memory efficiency
- HNSW tuning for faster search
- Sharding for scale

---
